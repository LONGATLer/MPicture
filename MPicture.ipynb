{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#必要运行库，saucenao_api代码需修改\n",
    "#pip install -U saucenao_api\n",
    "#pip install --upgrade urllib3\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from saucenao_api import SauceNao\n",
    "import time\n",
    "#gelbooru爬虫使用库\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "\n",
    "#gelbooru_headers\n",
    "gelbooru_headers = {\n",
    "     ''\n",
    "     }\n",
    "\n",
    "pixiv_headers = {\n",
    "     'Cookie':'',\n",
    "     'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0'\n",
    "     }\n",
    "\n",
    "\n",
    "#封装函数\n",
    "#gelbooru下载\n",
    "def gelbooru_download_picture(download_url,org_pic_url,picture_name,save_directory):#下载网页链接用于提取网页id打印结果，提取出的原图链接用于下载，图片名字，保存目录\n",
    "    #使用requests爬取\n",
    "    response = requests.get(org_pic_url,stream=True,headers=gelbooru_headers,timeout=20)#单线程，分块下载，超时时间20秒\n",
    "    if response.status_code == 200:\n",
    "            #文件名为默认图片名\n",
    "            file_save = os.path.join(save_directory, picture_name)\n",
    "            # 将响应内容写入文件\n",
    "            with open(file_save, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f'gelbooru的id：{download_url.split('=')[3]}，保存的文件名：{picture_name}，下载完成')\n",
    "    else:\n",
    "            print(f'无法下载网页{download_url.split('=')[3]}状态码：{response.status_code}')\n",
    "\n",
    "#gelbooru获取原图链接并下载\n",
    "def gelbooru_download(gelbooru_id,save_directory):\n",
    "    download_url = 'https://gelbooru.com/index.php?page=post&s=view&id='+ str(gelbooru_id)\n",
    "    website_text = requests.get(download_url,headers = gelbooru_headers).text#发送get请求并将内容转化成文本\n",
    "    soup = BeautifulSoup(website_text,features='lxml')#lxml解析网页内容\n",
    "    soup.find_all(id='image')[0]['src']#找到含有id='image'内容的\n",
    "    picture_site = soup.find_all(id='image')[0]['src'].split('/')#用/切分sample网站\n",
    "    if picture_site[4] == 'images':\n",
    "        org_pic_url = '/'.join(picture_site)#原图网址\n",
    "        gelbooru_download_picture(download_url,org_pic_url,picture_site[7],save_directory)\n",
    "    if picture_site[4] == 'samples':\n",
    "        picture_site[7] = picture_site[7].split('_')[1]#修改切分后内容，改为原图网址\n",
    "        picture_site[4] = 'images'\n",
    "        org_pic_url = '/'.join(picture_site)#原图网址\n",
    "        gelbooru_download_picture(download_url,org_pic_url,picture_site[7],save_directory)\n",
    "\n",
    "#gelbooru获取Source网址\n",
    "def get_gelbooru_source(gelbooru_id):\n",
    "    download_url = 'https://gelbooru.com/index.php?page=post&s=view&id='+ str(gelbooru_id)\n",
    "    website_text = requests.get(download_url,headers = gelbooru_headers).text#发送get请求并将内容转化成文本\n",
    "    soup = BeautifulSoup(website_text,features='lxml')#lxml解析网页内容\n",
    "    source_site = soup.find_all('a',rel=\"nofollow\")[0]['href']#获取到pixiv的id\n",
    "    return source_site\n",
    "\n",
    "#从gelbooru获取的Source网址中提取出对应推特网址或者pixiv的id\n",
    "def get_gelbooru_source_id(source_site):\n",
    "    pixiv_id = '非pixiv源'\n",
    "    twitter_website = '非twitter源'\n",
    "    if 'www.pixiv.net' in source_site.split('/'):\n",
    "        pixiv_id = source_site.split('/')[4]#pixiv的id\n",
    "    if 'twitter.com' in source_site.split('/'):\n",
    "        twitter_website = source_site#推特记录网址\n",
    "    return pixiv_id,twitter_website\n",
    "\n",
    "#提取搜索结果\n",
    "def get_ulr_id(url_pool):\n",
    "    pixiv_results=[]\n",
    "    gelbooru_results=[]\n",
    "    twitter_results=[]\n",
    "    for url in url_pool:#对池中url依次判断是否为需要的网站\n",
    "        if 'pixiv' in url.split('.'):\n",
    "            if url.split('=')[2] not in pixiv_results:#结果中可能有多个p站链接\n",
    "                pixiv_results.append(url.split('=')[2])\n",
    "        if 'twitter.com' in url.split('/'):\n",
    "            if url not in twitter_results:#结果中可能有多个链接\n",
    "                twitter_website.append(url)\n",
    "        if 'gelbooru.com' in url.split('/'):\n",
    "            if url.split('=')[3] not in gelbooru_results:#结果中可能有多个链接\n",
    "                gelbooru_results.append(url.split('=')[3])\n",
    "            pixiv_id,twitter_website = get_gelbooru_source_id(get_gelbooru_source(url.split('=')[3]))#先通过get_gelbooru_source函数得到来源，再用get_gelbooru_source_id得到id\n",
    "            if pixiv_id not in pixiv_results and pixiv_id != '非pixiv源':#get_gelbooru_source_id函数默认返回\n",
    "                pixiv_results.append(pixiv_id)#将得到的p站id再次加入pixiv_results\n",
    "            if twitter_website not in twitter_results and twitter_website != '非twitter源':#get_gelbooru_source_id函数默认返回\n",
    "                twitter_results.append(twitter_website)#将得到的推特网址再次加入twitter_results\n",
    "    return pixiv_results,gelbooru_results,twitter_results\n",
    "\n",
    "def SauceNao_Search(picture_path,sauce_api):#待搜索图片路径，搜索api\n",
    "    #最终结果存储数组\n",
    "    pixiv_id_results=[]\n",
    "    gelbooru_id_results=[]\n",
    "    twitter_website_results=[]\n",
    "    search_picture_list=[]\n",
    "    now_search_picture = ' '#存储当前搜索图片名称\n",
    "    #os.listdir()方法获取文件夹名字，返回数组\n",
    "    count=0#搜索次数\n",
    "    file_name_list = os.scandir(picture_path)\n",
    "    for file in file_name_list:\n",
    "        if file.is_dir() == False:\n",
    "            search_picture_list.append(file)#添加搜索完成的图片名\n",
    "            now_search_picture = file.name#保存当前搜索的文件名\n",
    "            url_pool = []#将本轮results搜索到的链接全部加入其中，放在循环中，每轮清空\n",
    "            sauce_search_results = SauceNao(sauce_api).from_file(picture_path + '\\\\' + file.name)  # or from_url()\n",
    "            time.sleep(10)#每次搜索之间间隔\n",
    "            for source in sauce_search_results:#提取档次搜索结果所有网址到url_pool\n",
    "                if source.similarity > 65:#判断相似度大于x的搜索结果，并将结果加入url_pool\n",
    "                        url_pool.extend(source.urls)\n",
    "            pixiv_id_results.extend(get_ulr_id(url_pool)[0])\n",
    "            gelbooru_id_results.extend(get_ulr_id(url_pool)[1])\n",
    "            twitter_website_results.extend(get_ulr_id(url_pool)[2])\n",
    "            count=count+1\n",
    "            print(f'{count}图完成：{now_search_picture}，pixiv_id：{get_ulr_id(url_pool)[0]}，gelbooru_id：{get_ulr_id(url_pool)[1]}，推特：{get_ulr_id(url_pool)[2]}')\n",
    "    return search_picture_list,pixiv_id_results,gelbooru_id_results,twitter_website_results,now_search_picture\n",
    "    #返回结果依次为已搜索图片，pixiv搜索到的id，gelbooru搜索到的id，pixiv网址，gelbooru网址，当前搜索的图片名称\n",
    "\n",
    "#p站关注爬取函数\n",
    "def get_pixiv_following(id,pixiv_headers):\n",
    "    #默认参数，不需要改动；从offset开始，得到limit的个数，所以offset可以为任意值，limit为100，单次得到offest+limit范围内的关注\n",
    "    limit = 100\n",
    "    rest = 'show'\n",
    "    tag = ''\n",
    "    lang = 'zh'\n",
    "    user_ids = []#保存关注id\n",
    "    following_numbers = json.loads(requests.get(f'https://www.pixiv.net/ajax/user/extra?lang={lang}',headers = pixiv_headers).text)['body']['following']#获取关注数量\n",
    "    for offset in range(following_numbers//100 + 1):\n",
    "        following_response = requests.get(f'https://www.pixiv.net/ajax/user/{id}/following?offset={offset*100}&limit={limit}&rest={rest}&tag={tag}&acceptingRequests=0&lang={lang}',headers = pixiv_headers).text#单次循环得到100个关注\n",
    "        time.sleep(1)#每次之间间隔，防止请求太频繁\n",
    "        following_json_data = json.loads(following_response)#json库解析内容\n",
    "        for user in following_json_data[\"body\"][\"users\"]:\n",
    "            user_ids.append(user[\"userId\"])\n",
    "    return user_ids\n",
    "\n",
    "#得到单张p站图的作者id\n",
    "def get_pixiv_author(picture_id):\n",
    "    picture_authorid = json.loads(requests.get(f'https://www.pixiv.net/ajax/illust/{picture_id}',headers = pixiv_headers).text)['body']['tags']['authorId']\n",
    "    return picture_authorid\n",
    "\n",
    "#系统级代理\n",
    "os.environ[\"http_proxy\"] = 'http://127.0.0.1:7890'\n",
    "os.environ[\"https_proxy\"] = 'http://127.0.0.1:7890'\n",
    "Image.MAX_IMAGE_PIXELS = None# 关闭DecompressionBombWarning警告\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,pixiv_id_results,gelbooru_id_results,twitter_website_results,now_search_picture = SauceNao_Search(r'F:\\Picture\\Search\\now','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixiv_id_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in pixiv_id_results: print(id)#打印p站id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gelbooru爬取上面得到的图片id\n",
    "gid=0\n",
    "for id in gelbooru_id_results:\n",
    "    if id != gelbooru_id_results[0] or id != gelbooru_id_results[-1]:#非首尾文件暂停时间\n",
    "        time.sleep(8)#每次间隔\n",
    "        gelbooru_download(id,r'C:\\Users\\13213\\Downloads\\gelbooru')\n",
    "    gid=gid+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#当前搜索文件名\n",
    "now_search_picture.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            调试区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pool=[]\n",
    "for source in sauce_search_results:#提取档次搜索结果所有网址到url_pool\n",
    "    if source.similarity > 65:#判断相似度大于x的搜索结果，并将结果加入url_pool\n",
    "        url_pool.extend(source.urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ulr_id(url_pool):\n",
    "    pixiv_results=[]\n",
    "    gelbooru_results=[]\n",
    "    twitter_results=[]\n",
    "    c=0\n",
    "    for url in url_pool:#对池中url依次判断是否为需要的网站\n",
    "        if 'pixiv' in url.split('.'):\n",
    "            if url.split('=')[2] not in pixiv_results:\n",
    "            print(c,pixiv_results)\n",
    "            pixiv_results.append(url.split('=')[2])\n",
    "            print(c,pixiv_results)\n",
    "        if 'twitter.com' in url.split('/'):\n",
    "            twitter_website.append(url)\n",
    "        if 'gelbooru.com' in url.split('/'):\n",
    "            gelbooru_results.append(url.split('=')[3])\n",
    "            pixiv_id,twitter_website = get_gelbooru_source_id(get_gelbooru_source(url.split('=')[3]))#先通过get_gelbooru_source函数得到来源，再用get_gelbooru_source_id得到id\n",
    "            print(c,'if前',pixiv_results)\n",
    "            if pixiv_id not in pixiv_results and pixiv_id != '非pixiv源':#get_gelbooru_source_id函数默认返回\n",
    "                pixiv_results.append(pixiv_id)#将得到的p站id再次加入pixiv_results\n",
    "            print(c,'if后',pixiv_results)\n",
    "            if twitter_website not in twitter_results and twitter_website != '非twitter源':#get_gelbooru_source_id函数默认返回\n",
    "                twitter_results.append(twitter_website)#将得到的推特网址再次加入twitter_results\n",
    "        c=c+1\n",
    "    return pixiv_results,gelbooru_results,twitter_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc():\n",
    "    return 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = abc()[0]\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
